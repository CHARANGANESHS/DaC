{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms as T, models\n",
    "from tqdm import tqdm\n",
    "import medmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting source training on OrganSMNIST train split...\n",
      "Using downloaded and verified file: /home/charanganeshcharanganesh/.medmnist/organsmnist.npz\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 188\u001b[39m\n\u001b[32m    185\u001b[39m source_subset = Subset(source_dataset_full, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[32m500\u001b[39m, \u001b[38;5;28mlen\u001b[39m(source_dataset_full)))))\n\u001b[32m    186\u001b[39m source_loader = DataLoader(source_subset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers=\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m source_model = \u001b[43mSFUDAModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m optimizer_source = optim.SGD(source_model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=\u001b[32m5e-4\u001b[39m)\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(source_epochs):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/AI/lib/python3.12/site-packages/torch/nn/modules/module.py:1174\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1171\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1172\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/AI/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    778\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    779\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m780\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    784\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    785\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    790\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    791\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/AI/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    778\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    779\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m780\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    784\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    785\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    790\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    791\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/AI/lib/python3.12/site-packages/torch/nn/modules/module.py:805\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    803\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m805\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    806\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    808\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/AI/lib/python3.12/site-packages/torch/nn/modules/module.py:1160\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1153\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1154\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1155\u001b[39m             device,\n\u001b[32m   1156\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1157\u001b[39m             non_blocking,\n\u001b[32m   1158\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1159\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from medmnist import INFO\n",
    "\n",
    "# ================================\n",
    "# Configuration and Random Seeds\n",
    "# ================================\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "batch_size = 32\n",
    "source_epochs = 5   # For quick testing – increase for full training\n",
    "adapt_epochs = 5\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "# Data flags for source and target\n",
    "source_flag = 'organsmnist'  # Source dataset: OrganSMNIST\n",
    "target_flag = 'organamnist'  # Target dataset: OrganAMNIST\n",
    "num_classes = 11  # Both should have same number of classes\n",
    "\n",
    "# ====================================\n",
    "# Define Transforms (No ToPILImage Call)\n",
    "# ====================================\n",
    "# Since MedMNIST returns PIL Images, we do not call ToPILImage in our transforms.\n",
    "source_transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "weak_transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "strong_transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# ========================================================\n",
    "# Custom Dataset for Target Adaptation (Dual Augmentation)\n",
    "# ========================================================\n",
    "class OrganTargetDataset(Dataset):\n",
    "    def __init__(self, dataset, transform_weak, transform_strong):\n",
    "        self.dataset = dataset  # Assumes dataset returns (img, label) with img as PIL Image.\n",
    "        self.transform_weak = transform_weak\n",
    "        self.transform_strong = transform_strong\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        img_w = self.transform_weak(img)\n",
    "        img_s = self.transform_strong(img)\n",
    "        return img_w, img_s, int(label)\n",
    "\n",
    "# ==================================\n",
    "# Model and Memory Bank Definitions\n",
    "# ==================================\n",
    "class SFUDAModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SFUDAModel, self).__init__()\n",
    "        backbone = models.resnet50(pretrained=True)\n",
    "        # Remove the last FC layer.\n",
    "        self.feature_extractor = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        self.fc = nn.Linear(backbone.fc.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # If input has one channel, repeat to 3 channels.\n",
    "        if x.size(1) == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        logits = self.fc(features)\n",
    "        return features, logits\n",
    "\n",
    "class MemoryBank:\n",
    "    def __init__(self, feature_dim, dataset_size, momentum=0.2):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.momentum = momentum\n",
    "        self.bank = torch.zeros(dataset_size, feature_dim).to(device)\n",
    "    \n",
    "    def update(self, indices, features):\n",
    "        with torch.no_grad():\n",
    "            self.bank[indices] = self.momentum * self.bank[indices] + (1 - self.momentum) * features\n",
    "            self.bank[indices] = F.normalize(self.bank[indices], dim=1)\n",
    "    \n",
    "    def get_features(self):\n",
    "        return self.bank\n",
    "\n",
    "# ====================================\n",
    "# Loss Functions and Pseudo-labeling Tools\n",
    "# ====================================\n",
    "def self_training_loss(p_w, p_s, pseudo_label, num_classes, lambda_diversity=0.1, omega_entropy=0.1):\n",
    "    ce_loss = F.cross_entropy(p_w, pseudo_label.long()) + F.cross_entropy(p_s, pseudo_label.long())\n",
    "    mean_pred = p_w.mean(dim=0)\n",
    "    diversity_loss = F.kl_div(mean_pred.log(), torch.full_like(mean_pred, 1/num_classes), reduction='batchmean')\n",
    "    entropy_loss = - (p_w * p_w.log()).sum(dim=1).mean()\n",
    "    return ce_loss + lambda_diversity * diversity_loss + omega_entropy * entropy_loss\n",
    "\n",
    "def contrastive_loss(anchor, positive, negatives, temperature=0.05):\n",
    "    anchor = F.normalize(anchor, dim=1)\n",
    "    positive = F.normalize(positive, dim=1)\n",
    "    negatives = F.normalize(negatives, dim=1)\n",
    "    \n",
    "    pos_sim = torch.sum(anchor * positive, dim=1) / temperature\n",
    "    neg_sim = torch.matmul(anchor, negatives.t()) / temperature\n",
    "    logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)\n",
    "    labels = torch.zeros(anchor.size(0), dtype=torch.long).to(device)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    return loss\n",
    "\n",
    "def mmd_loss(source_features, target_features):\n",
    "    mean_source = source_features.mean(dim=0)\n",
    "    mean_target = target_features.mean(dim=0)\n",
    "    loss = torch.norm(mean_source - mean_target, p=2)**2\n",
    "    return loss\n",
    "\n",
    "def compute_centroids(features, pseudo_labels, num_classes):\n",
    "    centroids = []\n",
    "    for c in range(num_classes):\n",
    "        mask = (pseudo_labels == c)\n",
    "        if mask.sum() == 0:\n",
    "            centroid = torch.zeros(features.size(1)).to(device)\n",
    "        else:\n",
    "            centroid = features[mask].mean(dim=0)\n",
    "            centroid = F.normalize(centroid.unsqueeze(0), dim=1).squeeze(0)\n",
    "        centroids.append(centroid)\n",
    "    centroids = torch.stack(centroids, dim=0)\n",
    "    return centroids\n",
    "\n",
    "def assign_pseudo_labels(features, centroids):\n",
    "    features = F.normalize(features, dim=1)\n",
    "    sim = torch.matmul(features, centroids.t())\n",
    "    pseudo_labels = sim.argmax(dim=1)\n",
    "    confidence, _ = sim.max(dim=1)\n",
    "    return pseudo_labels, confidence\n",
    "\n",
    "def divide_samples(confidence, threshold=0.8):\n",
    "    source_like_idx = (confidence >= threshold).nonzero(as_tuple=False).squeeze()\n",
    "    target_specific_idx = (confidence < threshold).nonzero(as_tuple=False).squeeze()\n",
    "    return source_like_idx, target_specific_idx\n",
    "\n",
    "def update_pseudo_labels(model, dataloader, tau=0.8):\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_indices = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (img_w, _, _) in enumerate(dataloader):\n",
    "            img_w = img_w.to(device)\n",
    "            features, _ = model(img_w)\n",
    "            features = F.normalize(features, dim=1)\n",
    "            all_features.append(features)\n",
    "            indices = torch.arange(batch_idx * batch_size, min((batch_idx+1)*batch_size, len(dataloader.dataset))).to(device)\n",
    "            all_indices.append(indices)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_indices = torch.cat(all_indices, dim=0)\n",
    "    pseudo_labels = all_features.argmax(dim=1)  # initial pseudo-labels\n",
    "    centroids = compute_centroids(all_features, pseudo_labels, num_classes)\n",
    "    pseudo_labels, confidence = assign_pseudo_labels(all_features, centroids)\n",
    "    source_like_idx, target_specific_idx = divide_samples(confidence, threshold=tau)\n",
    "    model.train()\n",
    "    return all_indices, pseudo_labels, confidence, source_like_idx, target_specific_idx, centroids\n",
    "\n",
    "# ====================================\n",
    "# Main: Source Training and Adaptation\n",
    "# ====================================\n",
    "if __name__ == \"__main__\":\n",
    "    # ----------------------------\n",
    "    # 1. Source Training on OrganSMNIST\n",
    "    # ----------------------------\n",
    "    print(\"Starting source training on OrganSMNIST train split...\")\n",
    "    DataClass_source = getattr(medmnist, INFO[source_flag]['python_class'])\n",
    "    source_dataset_full = DataClass_source(split='train', transform=source_transform, download=True)\n",
    "    source_subset = Subset(source_dataset_full, list(range(min(500, len(source_dataset_full)))))\n",
    "    source_loader = DataLoader(source_subset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    source_model = SFUDAModel(num_classes=num_classes).to(device)\n",
    "    optimizer_source = optim.SGD(source_model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=5e-4)\n",
    "    \n",
    "    for epoch in range(source_epochs):\n",
    "        running_loss = 0.0\n",
    "        for img, labels in tqdm(source_loader, desc=f\"Source Epoch {epoch+1}/{source_epochs}\", ncols=80):\n",
    "            img = img.to(device)\n",
    "            labels = labels.squeeze().to(device)\n",
    "            features, logits = source_model(img)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            optimizer_source.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_source.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Source Epoch [{epoch+1}/{source_epochs}], Loss: {running_loss/len(source_loader):.4f}\")\n",
    "    \n",
    "    torch.save(source_model.state_dict(), \"../models/source_model_organsmnist.pth\")\n",
    "    print(\"Source training finished and model saved.\")\n",
    "    \n",
    "    # ----------------------------\n",
    "    # 2. Adaptation on OrganAMNIST Test Split\n",
    "    # ----------------------------\n",
    "    # Load pretrained source model weights into a new model instance.\n",
    "    adapted_model = SFUDAModel(num_classes=num_classes).to(device)\n",
    "    adapted_model.load_state_dict(torch.load(\"../models/source_model_organsmnist.pth\"))\n",
    "    # Freeze the classifier head (as common in source-free DA)\n",
    "    for param in adapted_model.fc.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    DataClass_target = getattr(medmnist, INFO[target_flag]['python_class'])\n",
    "    target_dataset_full = DataClass_target(split='test', transform=None, download=True)\n",
    "    target_dataset = OrganTargetDataset(target_dataset_full, transform_weak=weak_transform, transform_strong=strong_transform)\n",
    "    target_subset = Subset(target_dataset, list(range(min(500, len(target_dataset)))))\n",
    "    target_loader = DataLoader(target_subset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    dataset_size = len(target_subset)\n",
    "    feature_dim = 2048\n",
    "    memory_bank = MemoryBank(feature_dim, dataset_size, momentum=0.2)\n",
    "    \n",
    "    optimizer_adapt = optim.SGD(adapted_model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=5e-4)\n",
    "    alpha = 1.0  # weight for self-training loss\n",
    "    beta = 0.5   # weight for MMD loss\n",
    "    gamma = 1.0  # weight for contrastive loss\n",
    "    dataset_indices = np.arange(dataset_size)\n",
    "    \n",
    "    print(\"Starting adaptation on OrganAMNIST test split...\")\n",
    "    for epoch in range(adapt_epochs):\n",
    "        all_indices, pseudo_labels, confidence, src_like_idx, tgt_spec_idx, centroids = update_pseudo_labels(adapted_model, target_loader, tau=0.8)\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (img_w, img_s, _) in enumerate(tqdm(target_loader, desc=f\"Adapt Epoch {epoch+1}/{adapt_epochs}\", ncols=80)):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = batch_start + img_w.size(0)\n",
    "            batch_indices = torch.tensor(dataset_indices[batch_start:batch_end]).to(device)\n",
    "            \n",
    "            img_w = img_w.to(device)\n",
    "            img_s = img_s.to(device)\n",
    "            \n",
    "            features_w, logits_w = adapted_model(img_w)\n",
    "            features_s, logits_s = adapted_model(img_s)\n",
    "            p_w = F.softmax(logits_w, dim=1)\n",
    "            p_s = F.softmax(logits_s, dim=1)\n",
    "            batch_pseudo = pseudo_labels[batch_indices]\n",
    "            \n",
    "            loss_self = self_training_loss(p_w, p_s, batch_pseudo, num_classes)\n",
    "            batch_features = F.normalize(features_w, dim=1)\n",
    "            positive_proto = centroids[batch_pseudo]\n",
    "            bank_features = memory_bank.get_features()\n",
    "            \n",
    "            loss_contrastive = 0.0\n",
    "            for i in range(batch_features.size(0)):\n",
    "                neg_mask = (pseudo_labels != batch_pseudo[i])\n",
    "                negatives = bank_features[neg_mask]\n",
    "                if negatives.size(0) > 0:\n",
    "                    loss_contrastive += contrastive_loss(batch_features[i].unsqueeze(0),\n",
    "                                                         positive_proto[i].unsqueeze(0),\n",
    "                                                         negatives)\n",
    "            loss_contrastive = loss_contrastive / batch_features.size(0)\n",
    "            \n",
    "            src_mask = torch.tensor([idx.item() in src_like_idx.cpu().numpy() for idx in batch_indices]).bool()\n",
    "            tgt_mask = torch.tensor([idx.item() in tgt_spec_idx.cpu().numpy() for idx in batch_indices]).bool()\n",
    "            if src_mask.sum() > 0 and tgt_mask.sum() > 0:\n",
    "                src_feats = batch_features[src_mask]\n",
    "                tgt_feats = batch_features[tgt_mask]\n",
    "                loss_mmd = mmd_loss(src_feats, tgt_feats)\n",
    "            else:\n",
    "                loss_mmd = torch.tensor(0.0).to(device)\n",
    "            \n",
    "            loss = alpha * loss_self + gamma * loss_contrastive + beta * loss_mmd\n",
    "            \n",
    "            optimizer_adapt.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_adapt.step()\n",
    "            \n",
    "            memory_bank.update(batch_indices, batch_features.detach())\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Adapt Epoch [{epoch+1}/{adapt_epochs}], Loss: {running_loss/len(target_loader):.4f}\")\n",
    "    \n",
    "    # ----------------------------\n",
    "    # 3. Evaluation on Target\n",
    "    # ----------------------------\n",
    "    def evaluate(model, dataloader):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for img_w, _, labels in dataloader:\n",
    "                img_w = img_w.to(device)\n",
    "                labels = labels.to(device)\n",
    "                _, logits = model(img_w)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return 100.0 * correct / total\n",
    "    \n",
    "    accuracy = evaluate(adapted_model, target_loader)\n",
    "    print(f\"Adapted Target Domain Accuracy on OrganAMNIST Test Subset: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting source training on OrganSMNIST train split...\n",
      "Using downloaded and verified file: /home/charanganeshcharanganesh/.medmnist/organsmnist.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source Epoch 1/5: 100%|█████████████████████████| 16/16 [01:01<00:00,  3.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Epoch [1/5], Loss: 2.1521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source Epoch 2/5: 100%|█████████████████████████| 16/16 [01:01<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Epoch [2/5], Loss: 1.8182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source Epoch 3/5: 100%|█████████████████████████| 16/16 [01:00<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Epoch [3/5], Loss: 1.5313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source Epoch 4/5: 100%|█████████████████████████| 16/16 [01:00<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Epoch [4/5], Loss: 1.3227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source Epoch 5/5: 100%|█████████████████████████| 16/16 [01:00<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Epoch [5/5], Loss: 1.2012\n",
      "Source training finished and model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21757/566999470.py:224: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapted_model.load_state_dict(torch.load(\"source_model_organsmnist.pth\"))\n",
      "/tmp/ipykernel_21757/566999470.py:78: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return img_w, img_s, int(label)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/charanganeshcharanganesh/.medmnist/organamnist.npz\n",
      "Starting adaptation on OrganAMNIST test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapt Epoch 1/5: 100%|██████████████████████████| 16/16 [02:09<00:00,  8.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapt Epoch [1/5], Loss: 4.8452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapt Epoch 2/5: 100%|██████████████████████████| 16/16 [02:30<00:00,  9.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapt Epoch [2/5], Loss: 5.0878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapt Epoch 3/5:  81%|█████████████████████▏    | 13/16 [01:45<00:24,  8.15s/it]IOStream.flush timed out\n",
      "Adapt Epoch 3/5: 100%|██████████████████████████| 16/16 [02:06<00:00,  7.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapt Epoch [3/5], Loss: 4.3850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapt Epoch 4/5: 100%|██████████████████████████| 16/16 [02:29<00:00,  9.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapt Epoch [4/5], Loss: 3.8401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapt Epoch 5/5: 100%|██████████████████████████| 16/16 [02:07<00:00,  7.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapt Epoch [5/5], Loss: 3.5663\n",
      "Adapted Target Domain Accuracy on OrganAMNIST Test Subset: 7.20%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directly import the datasets\n",
    "from medmnist import OrganSMNIST, OrganAMNIST\n",
    "\n",
    "# ================================\n",
    "# Configuration and Random Seeds\n",
    "# ================================\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "batch_size = 32\n",
    "source_epochs = 5   # For testing; increase for full training\n",
    "adapt_epochs = 5\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "# Both datasets should have the same number of classes.\n",
    "# For MedMNIST, OrganSMNIST and OrganAMNIST share the same label space.\n",
    "num_classes = 11\n",
    "\n",
    "# ====================================\n",
    "# Define Transforms (No ToPILImage Call)\n",
    "# ====================================\n",
    "# MedMNIST returns PIL Images, so we don't need ToPILImage.\n",
    "source_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# ========================================================\n",
    "# Custom Dataset for Target Adaptation (Dual Augmentation)\n",
    "# ========================================================\n",
    "class OrganTargetDataset(Dataset):\n",
    "    def __init__(self, dataset, transform_weak, transform_strong):\n",
    "        self.dataset = dataset  # dataset returns (img, label) with img as a PIL Image.\n",
    "        self.transform_weak = transform_weak\n",
    "        self.transform_strong = transform_strong\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        img_w = self.transform_weak(img)\n",
    "        img_s = self.transform_strong(img)\n",
    "        return img_w, img_s, int(label)\n",
    "\n",
    "# ==================================\n",
    "# Model and Memory Bank Definitions\n",
    "# ==================================\n",
    "class SFUDAModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SFUDAModel, self).__init__()\n",
    "        backbone = models.resnet50(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        self.fc = nn.Linear(backbone.fc.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.size(1) == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        logits = self.fc(features)\n",
    "        return features, logits\n",
    "\n",
    "class MemoryBank:\n",
    "    def __init__(self, feature_dim, dataset_size, momentum=0.2):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.momentum = momentum\n",
    "        self.bank = torch.zeros(dataset_size, feature_dim).to(device)\n",
    "    \n",
    "    def update(self, indices, features):\n",
    "        with torch.no_grad():\n",
    "            self.bank[indices] = self.momentum * self.bank[indices] + (1 - self.momentum) * features\n",
    "            self.bank[indices] = F.normalize(self.bank[indices], dim=1)\n",
    "    \n",
    "    def get_features(self):\n",
    "        return self.bank\n",
    "\n",
    "# ====================================\n",
    "# Loss Functions and Pseudo-labeling Tools\n",
    "# ====================================\n",
    "def self_training_loss(p_w, p_s, pseudo_label, num_classes, lambda_diversity=0.1, omega_entropy=0.1):\n",
    "    ce_loss = F.cross_entropy(p_w, pseudo_label.long()) + F.cross_entropy(p_s, pseudo_label.long())\n",
    "    mean_pred = p_w.mean(dim=0)\n",
    "    diversity_loss = F.kl_div(mean_pred.log(), torch.full_like(mean_pred, 1/num_classes), reduction='batchmean')\n",
    "    entropy_loss = - (p_w * p_w.log()).sum(dim=1).mean()\n",
    "    return ce_loss + lambda_diversity * diversity_loss + omega_entropy * entropy_loss\n",
    "\n",
    "def contrastive_loss(anchor, positive, negatives, temperature=0.05):\n",
    "    anchor = F.normalize(anchor, dim=1)\n",
    "    positive = F.normalize(positive, dim=1)\n",
    "    negatives = F.normalize(negatives, dim=1)\n",
    "    \n",
    "    pos_sim = torch.sum(anchor * positive, dim=1) / temperature\n",
    "    neg_sim = torch.matmul(anchor, negatives.t()) / temperature\n",
    "    logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)\n",
    "    labels = torch.zeros(anchor.size(0), dtype=torch.long).to(device)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    return loss\n",
    "\n",
    "def mmd_loss(source_features, target_features):\n",
    "    mean_source = source_features.mean(dim=0)\n",
    "    mean_target = target_features.mean(dim=0)\n",
    "    loss = torch.norm(mean_source - mean_target, p=2)**2\n",
    "    return loss\n",
    "\n",
    "def compute_centroids(features, pseudo_labels, num_classes):\n",
    "    centroids = []\n",
    "    for c in range(num_classes):\n",
    "        mask = (pseudo_labels == c)\n",
    "        if mask.sum() == 0:\n",
    "            centroid = torch.zeros(features.size(1)).to(device)\n",
    "        else:\n",
    "            centroid = features[mask].mean(dim=0)\n",
    "            centroid = F.normalize(centroid.unsqueeze(0), dim=1).squeeze(0)\n",
    "        centroids.append(centroid)\n",
    "    centroids = torch.stack(centroids, dim=0)\n",
    "    return centroids\n",
    "\n",
    "def assign_pseudo_labels(features, centroids):\n",
    "    features = F.normalize(features, dim=1)\n",
    "    sim = torch.matmul(features, centroids.t())\n",
    "    pseudo_labels = sim.argmax(dim=1)\n",
    "    confidence, _ = sim.max(dim=1)\n",
    "    return pseudo_labels, confidence\n",
    "\n",
    "def divide_samples(confidence, threshold=0.8):\n",
    "    source_like_idx = (confidence >= threshold).nonzero(as_tuple=False).squeeze()\n",
    "    target_specific_idx = (confidence < threshold).nonzero(as_tuple=False).squeeze()\n",
    "    return source_like_idx, target_specific_idx\n",
    "\n",
    "def update_pseudo_labels(model, dataloader, tau=0.8):\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_indices = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (img_w, _, _) in enumerate(dataloader):\n",
    "            img_w = img_w.to(device)\n",
    "            features, _ = model(img_w)\n",
    "            features = F.normalize(features, dim=1)\n",
    "            all_features.append(features)\n",
    "            indices = torch.arange(batch_idx * batch_size, min((batch_idx+1)*batch_size, len(dataloader.dataset))).to(device)\n",
    "            all_indices.append(indices)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_indices = torch.cat(all_indices, dim=0)\n",
    "    pseudo_labels = all_features.argmax(dim=1)  # initial pseudo-labels\n",
    "    centroids = compute_centroids(all_features, pseudo_labels, num_classes)\n",
    "    pseudo_labels, confidence = assign_pseudo_labels(all_features, centroids)\n",
    "    source_like_idx, target_specific_idx = divide_samples(confidence, threshold=tau)\n",
    "    model.train()\n",
    "    return all_indices, pseudo_labels, confidence, source_like_idx, target_specific_idx, centroids\n",
    "\n",
    "# ====================================\n",
    "# Main: Source Training and Adaptation\n",
    "# ====================================\n",
    "if __name__ == \"__main__\":\n",
    "    # ----------------------------\n",
    "    # 1. Source Training on OrganSMNIST Train Split\n",
    "    # ----------------------------\n",
    "    print(\"Starting source training on OrganSMNIST train split...\")\n",
    "    source_dataset_full = OrganSMNIST(split='train', transform=source_transform, download=True)\n",
    "    source_subset = Subset(source_dataset_full, list(range(min(500, len(source_dataset_full)))))\n",
    "    source_loader = DataLoader(source_subset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    source_model = SFUDAModel(num_classes=num_classes).to(device)\n",
    "    optimizer_source = optim.SGD(source_model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=5e-4)\n",
    "    \n",
    "    for epoch in range(source_epochs):\n",
    "        running_loss = 0.0\n",
    "        for img, labels in tqdm(source_loader, desc=f\"Source Epoch {epoch+1}/{source_epochs}\", ncols=80):\n",
    "            # print(torch.unique(labels))\n",
    "            # break\n",
    "            img = img.to(device)\n",
    "            labels = labels.squeeze().to(device)  # ensure labels are 1D\n",
    "            features, logits = source_model(img)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            optimizer_source.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_source.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Source Epoch [{epoch+1}/{source_epochs}], Loss: {running_loss/len(source_loader):.4f}\")\n",
    "    \n",
    "    torch.save(source_model.state_dict(), \"../models/source_model_organsmnist.pth\")\n",
    "    print(\"Source training finished and model saved.\")\n",
    "    \n",
    "    # ----------------------------\n",
    "    # 2. Adaptation on OrganAMNIST Test Split\n",
    "    # ----------------------------\n",
    "    # Load pretrained source model into a new model instance.\n",
    "    adapted_model = SFUDAModel(num_classes=num_classes).to(device)\n",
    "    adapted_model.load_state_dict(torch.load(\"../models/source_model_organsmnist.pth\"))\n",
    "    # Freeze the classifier head as per SFUDA.\n",
    "    for param in adapted_model.fc.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    target_dataset_full = OrganAMNIST(split='train', transform=None, download=True)\n",
    "    target_dataset = OrganTargetDataset(target_dataset_full, transform_weak=weak_transform, transform_strong=strong_transform)\n",
    "    target_subset = Subset(target_dataset, list(range(min(500, len(target_dataset)))))\n",
    "    target_loader = DataLoader(target_subset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    dataset_size = len(target_subset)\n",
    "    feature_dim = 2048\n",
    "    memory_bank = MemoryBank(feature_dim, dataset_size, momentum=0.2)\n",
    "    \n",
    "    optimizer_adapt = optim.SGD(adapted_model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=5e-4)\n",
    "    alpha = 1.0  # weight for self-training loss\n",
    "    beta = 0.5   # weight for MMD loss\n",
    "    gamma = 1.0  # weight for contrastive loss\n",
    "    dataset_indices = np.arange(dataset_size)\n",
    "    \n",
    "    print(\"Starting adaptation on OrganAMNIST test split...\")\n",
    "    for epoch in range(adapt_epochs):\n",
    "        all_indices, pseudo_labels, confidence, src_like_idx, tgt_spec_idx, centroids = update_pseudo_labels(adapted_model, target_loader, tau=0.8)\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (img_w, img_s, _) in enumerate(tqdm(target_loader, desc=f\"Adapt Epoch {epoch+1}/{adapt_epochs}\", ncols=80)):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = batch_start + img_w.size(0)\n",
    "            batch_indices = torch.tensor(dataset_indices[batch_start:batch_end]).to(device)\n",
    "            \n",
    "            img_w = img_w.to(device)\n",
    "            img_s = img_s.to(device)\n",
    "            \n",
    "            features_w, logits_w = adapted_model(img_w)\n",
    "            features_s, logits_s = adapted_model(img_s)\n",
    "            p_w = F.softmax(logits_w, dim=1)\n",
    "            p_s = F.softmax(logits_s, dim=1)\n",
    "            batch_pseudo = pseudo_labels[batch_indices]\n",
    "            \n",
    "            loss_self = self_training_loss(p_w, p_s, batch_pseudo, num_classes)\n",
    "            batch_features = F.normalize(features_w, dim=1)\n",
    "            positive_proto = centroids[batch_pseudo]\n",
    "            bank_features = memory_bank.get_features()\n",
    "            \n",
    "            loss_contrastive = 0.0\n",
    "            for i in range(batch_features.size(0)):\n",
    "                neg_mask = (pseudo_labels != batch_pseudo[i])\n",
    "                negatives = bank_features[neg_mask]\n",
    "                if negatives.size(0) > 0:\n",
    "                    loss_contrastive += contrastive_loss(batch_features[i].unsqueeze(0),\n",
    "                                                         positive_proto[i].unsqueeze(0),\n",
    "                                                         negatives)\n",
    "            loss_contrastive = loss_contrastive / batch_features.size(0)\n",
    "            \n",
    "            src_mask = torch.tensor([idx.item() in src_like_idx.cpu().numpy() for idx in batch_indices]).bool()\n",
    "            tgt_mask = torch.tensor([idx.item() in tgt_spec_idx.cpu().numpy() for idx in batch_indices]).bool()\n",
    "            if src_mask.sum() > 0 and tgt_mask.sum() > 0:\n",
    "                src_feats = batch_features[src_mask]\n",
    "                tgt_feats = batch_features[tgt_mask]\n",
    "                loss_mmd = mmd_loss(src_feats, tgt_feats)\n",
    "            else:\n",
    "                loss_mmd = torch.tensor(0.0).to(device)\n",
    "            \n",
    "            loss = alpha * loss_self + gamma * loss_contrastive + beta * loss_mmd\n",
    "            \n",
    "            optimizer_adapt.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_adapt.step()\n",
    "            \n",
    "            memory_bank.update(batch_indices, batch_features.detach())\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Adapt Epoch [{epoch+1}/{adapt_epochs}], Loss: {running_loss/len(target_loader):.4f}\")\n",
    "    \n",
    "    # ----------------------------\n",
    "    # 3. Evaluation on Target (OrganAMNIST)\n",
    "    # ----------------------------\n",
    "    def evaluate(model, dataloader):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for img_w, _, labels in dataloader:\n",
    "                img_w = img_w.to(device)\n",
    "                labels = labels.squeeze().to(device)\n",
    "                _, logits = model(img_w)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return 100.0 * correct / total\n",
    "    \n",
    "    accuracy = evaluate(adapted_model, target_loader)\n",
    "    print(f\"Adapted Target Domain Accuracy on OrganAMNIST Test Subset: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
