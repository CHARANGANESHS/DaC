{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm\n",
    "from medmnist import OrganSMNIST, OrganAMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "source_epochs = 5\n",
    "adapt_epochs = 5\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "num_classes = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrganTargetDataset(Dataset):\n",
    "    def __init__(self, dataset, transform_weak, transform_strong):\n",
    "        self.dataset = dataset\n",
    "        self.transform_weak = transform_weak\n",
    "        self.transform_strong = transform_strong\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        img_w = self.transform_weak(img)\n",
    "        img_s = self.transform_strong(img)\n",
    "        return img_w, img_s, int(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFUDAModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SFUDAModel, self).__init__()\n",
    "        backbone = models.resnet50(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        self.fc = nn.Linear(backbone.fc.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.size(1) == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        logits = self.fc(features)\n",
    "        return features, logits\n",
    "\n",
    "class MemoryBank:\n",
    "    def __init__(self, feature_dim, dataset_size, momentum=0.2):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.momentum = momentum\n",
    "        self.bank = torch.zeros(dataset_size, feature_dim).to(device)\n",
    "    \n",
    "    def update(self, indices, features):\n",
    "        with torch.no_grad():\n",
    "            self.bank[indices] = self.momentum * self.bank[indices] + (1 - self.momentum) * features\n",
    "            self.bank[indices] = F.normalize(self.bank[indices], dim=1)\n",
    "    \n",
    "    def get_features(self):\n",
    "        return self.bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_training_loss(p_w, p_s, pseudo_label, num_classes, lambda_diversity=0.1, omega_entropy=0.1):\n",
    "    ce_loss = F.cross_entropy(p_w, pseudo_label.long()) + F.cross_entropy(p_s, pseudo_label.long())\n",
    "    mean_pred = p_w.mean(dim=0)\n",
    "    diversity_loss = F.kl_div(mean_pred.log(), torch.full_like(mean_pred, 1/num_classes), reduction='batchmean')\n",
    "    entropy_loss = - (p_w * p_w.log()).sum(dim=1).mean()\n",
    "    return ce_loss + lambda_diversity * diversity_loss + omega_entropy * entropy_loss\n",
    "\n",
    "def contrastive_loss(anchor, positive, negatives, temperature=0.05):\n",
    "    anchor = F.normalize(anchor, dim=1)\n",
    "    positive = F.normalize(positive, dim=1)\n",
    "    negatives = F.normalize(negatives, dim=1)\n",
    "    \n",
    "    pos_sim = torch.sum(anchor * positive, dim=1) / temperature\n",
    "    neg_sim = torch.matmul(anchor, negatives.t()) / temperature\n",
    "    logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)\n",
    "    labels = torch.zeros(anchor.size(0), dtype=torch.long).to(device)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    return loss\n",
    "\n",
    "def mmd_loss(source_features, target_features):\n",
    "    mean_source = source_features.mean(dim=0)\n",
    "    mean_target = target_features.mean(dim=0)\n",
    "    loss = torch.norm(mean_source - mean_target, p=2)**2\n",
    "    return loss\n",
    "\n",
    "def compute_centroids(features, pseudo_labels, num_classes):\n",
    "    centroids = []\n",
    "    for c in range(num_classes):\n",
    "        mask = (pseudo_labels == c)\n",
    "        if mask.sum() == 0:\n",
    "            centroid = torch.zeros(features.size(1)).to(device)\n",
    "        else:\n",
    "            centroid = features[mask].mean(dim=0)\n",
    "            centroid = F.normalize(centroid.unsqueeze(0), dim=1).squeeze(0)\n",
    "        centroids.append(centroid)\n",
    "    centroids = torch.stack(centroids, dim=0)\n",
    "    return centroids\n",
    "\n",
    "def assign_pseudo_labels(features, centroids):\n",
    "    features = F.normalize(features, dim=1)\n",
    "    sim = torch.matmul(features, centroids.t())\n",
    "    pseudo_labels = sim.argmax(dim=1)\n",
    "    confidence, _ = sim.max(dim=1)\n",
    "    return pseudo_labels, confidence\n",
    "\n",
    "def divide_samples(confidence, threshold=0.8):\n",
    "    source_like_idx = (confidence >= threshold).nonzero(as_tuple=False).squeeze()\n",
    "    target_specific_idx = (confidence < threshold).nonzero(as_tuple=False).squeeze()\n",
    "    return source_like_idx, target_specific_idx\n",
    "\n",
    "def update_pseudo_labels(model, dataloader, tau=0.8):\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_indices = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (img_w, _, _) in enumerate(dataloader):\n",
    "            img_w = img_w.to(device)\n",
    "            features, _ = model(img_w)\n",
    "            features = F.normalize(features, dim=1)\n",
    "            all_features.append(features)\n",
    "            indices = torch.arange(batch_idx * batch_size, min((batch_idx+1)*batch_size, len(dataloader.dataset))).to(device)\n",
    "            all_indices.append(indices)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_indices = torch.cat(all_indices, dim=0)\n",
    "    pseudo_labels = all_features.argmax(dim=1)  # initial pseudo-labels\n",
    "    centroids = compute_centroids(all_features, pseudo_labels, num_classes)\n",
    "    pseudo_labels, confidence = assign_pseudo_labels(all_features, centroids)\n",
    "    source_like_idx, target_specific_idx = divide_samples(confidence, threshold=tau)\n",
    "    model.train()\n",
    "    return all_indices, pseudo_labels, confidence, source_like_idx, target_specific_idx, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting source training on OrganSMNIST train split...\n",
      "Using downloaded and verified file: /home/charanganeshcharanganesh/.medmnist/organsmnist.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charanganeshcharanganesh/miniconda3/envs/AI/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/charanganeshcharanganesh/miniconda3/envs/AI/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Source Epoch 1/5: 100%|███████████████████████| 436/436 [01:09<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Epoch [1/5], Loss: 1.1531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source Epoch 2/5: 100%|███████████████████████| 436/436 [01:08<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Epoch [2/5], Loss: 0.7998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source Epoch 3/5: 100%|███████████████████████| 436/436 [01:09<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Epoch [3/5], Loss: 0.7027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source Epoch 4/5: 100%|███████████████████████| 436/436 [01:09<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Epoch [4/5], Loss: 0.6427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source Epoch 5/5: 100%|███████████████████████| 436/436 [01:32<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Epoch [5/5], Loss: 0.6011\n",
      "Source training finished and model saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting source training on OrganSMNIST train split...\")\n",
    "source_dataset_full = OrganSMNIST(split='train', transform=source_transform, download=True)\n",
    "source_loader = DataLoader(source_dataset_full, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "source_model = SFUDAModel(num_classes=num_classes).to(device)\n",
    "optimizer_source = optim.SGD(source_model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=5e-4)\n",
    "\n",
    "for epoch in range(source_epochs):\n",
    "    running_loss = 0.0\n",
    "    for img, labels in tqdm(source_loader, desc=f\"Source Epoch {epoch+1}/{source_epochs}\", ncols=80):\n",
    "        # print(torch.unique(labels))\n",
    "        # break\n",
    "        img = img.to(device)\n",
    "        labels = labels.squeeze().to(device)\n",
    "        features, logits = source_model(img)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        optimizer_source.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_source.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Source Epoch [{epoch+1}/{source_epochs}], Loss: {running_loss/len(source_loader):.4f}\")\n",
    "\n",
    "torch.save(source_model.state_dict(), \"../models/source_model_organsmnist.pth\")\n",
    "print(\"Source training finished and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38029/3252064451.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapted_model.load_state_dict(torch.load(\"../models/source_model_organsmnist.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/charanganeshcharanganesh/.medmnist/organamnist.npz\n",
      "Starting adaptation on OrganAMNIST train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38029/689261903.py:14: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return img_w, img_s, int(label)\n",
      "Adapt Epoch 1/5: 100%|██████████████████████| 1081/1081 [05:43<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapt Epoch [1/5], Loss: 3.1954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapt Epoch 2/5: 100%|██████████████████████| 1081/1081 [05:19<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapt Epoch [2/5], Loss: 3.2385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapt Epoch 3/5: 100%|██████████████████████| 1081/1081 [05:19<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapt Epoch [3/5], Loss: 3.1385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapt Epoch 4/5: 100%|██████████████████████| 1081/1081 [05:18<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapt Epoch [4/5], Loss: 3.1373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapt Epoch 5/5: 100%|██████████████████████| 1081/1081 [05:25<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapt Epoch [5/5], Loss: 3.1368\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension specified as 0 but tensor has no dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     84\u001b[39m             total += labels.size(\u001b[32m0\u001b[39m)\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m100.0\u001b[39m * correct / total\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m accuracy = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapted_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAdapted Target Domain Accuracy on OrganAMNIST Test Subset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, dataloader)\u001b[39m\n\u001b[32m     82\u001b[39m         preds = logits.argmax(dim=\u001b[32m1\u001b[39m)\n\u001b[32m     83\u001b[39m         correct += (preds == labels).sum().item()\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m         total += \u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m100.0\u001b[39m * correct / total\n",
      "\u001b[31mIndexError\u001b[39m: Dimension specified as 0 but tensor has no dimensions"
     ]
    }
   ],
   "source": [
    "adapted_model = SFUDAModel(num_classes=num_classes).to(device)\n",
    "adapted_model.load_state_dict(torch.load(\"../models/source_model_organsmnist.pth\"))\n",
    "\n",
    "for param in adapted_model.fc.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "target_dataset_full = OrganAMNIST(split='train', transform=None, download=True)\n",
    "target_dataset = OrganTargetDataset(target_dataset_full, transform_weak=weak_transform, transform_strong=strong_transform)\n",
    "target_loader = DataLoader(target_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "dataset_size = len(target_dataset)\n",
    "feature_dim = 2048\n",
    "memory_bank = MemoryBank(feature_dim, dataset_size, momentum=0.2)\n",
    "\n",
    "optimizer_adapt = optim.SGD(adapted_model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=5e-4)\n",
    "alpha = 1.0\n",
    "beta = 0.5\n",
    "gamma = 1.0\n",
    "dataset_indices = np.arange(dataset_size)\n",
    "\n",
    "print(\"Starting adaptation on OrganAMNIST train split...\")\n",
    "for epoch in range(adapt_epochs):\n",
    "    all_indices, pseudo_labels, confidence, src_like_idx, tgt_spec_idx, centroids = update_pseudo_labels(adapted_model, target_loader, tau=0.8)\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (img_w, img_s, _) in enumerate(tqdm(target_loader, desc=f\"Adapt Epoch {epoch+1}/{adapt_epochs}\", ncols=80)):\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = batch_start + img_w.size(0)\n",
    "        batch_indices = torch.tensor(dataset_indices[batch_start:batch_end]).to(device)\n",
    "        \n",
    "        img_w = img_w.to(device)\n",
    "        img_s = img_s.to(device)\n",
    "        \n",
    "        features_w, logits_w = adapted_model(img_w)\n",
    "        features_s, logits_s = adapted_model(img_s)\n",
    "        p_w = F.softmax(logits_w, dim=1)\n",
    "        p_s = F.softmax(logits_s, dim=1)\n",
    "        batch_pseudo = pseudo_labels[batch_indices]\n",
    "        \n",
    "        loss_self = self_training_loss(p_w, p_s, batch_pseudo, num_classes)\n",
    "        batch_features = F.normalize(features_w, dim=1)\n",
    "        positive_proto = centroids[batch_pseudo]\n",
    "        bank_features = memory_bank.get_features()\n",
    "        \n",
    "        loss_contrastive = 0.0\n",
    "        for i in range(batch_features.size(0)):\n",
    "            neg_mask = (pseudo_labels != batch_pseudo[i])\n",
    "            negatives = bank_features[neg_mask]\n",
    "            if negatives.size(0) > 0:\n",
    "                loss_contrastive += contrastive_loss(batch_features[i].unsqueeze(0),\n",
    "                                                        positive_proto[i].unsqueeze(0),\n",
    "                                                        negatives)\n",
    "        loss_contrastive = loss_contrastive / batch_features.size(0)\n",
    "        \n",
    "        src_mask = torch.tensor([idx.item() in src_like_idx.cpu().numpy() for idx in batch_indices]).bool()\n",
    "        tgt_mask = torch.tensor([idx.item() in tgt_spec_idx.cpu().numpy() for idx in batch_indices]).bool()\n",
    "        if src_mask.sum() > 0 and tgt_mask.sum() > 0:\n",
    "            src_feats = batch_features[src_mask]\n",
    "            tgt_feats = batch_features[tgt_mask]\n",
    "            loss_mmd = mmd_loss(src_feats, tgt_feats)\n",
    "        else:\n",
    "            loss_mmd = torch.tensor(0.0).to(device)\n",
    "        \n",
    "        loss = alpha * loss_self + gamma * loss_contrastive + beta * loss_mmd\n",
    "        \n",
    "        optimizer_adapt.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_adapt.step()\n",
    "        \n",
    "        memory_bank.update(batch_indices, batch_features.detach())\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Adapt Epoch [{epoch+1}/{adapt_epochs}], Loss: {running_loss/len(target_loader):.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38029/689261903.py:14: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return img_w, img_s, int(label)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapted Target Domain Accuracy on OrganAMNIST Train Subset: 5.66%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for img_w, _, labels in dataloader:\n",
    "            img_w = img_w.to(device)\n",
    "            if labels.dim() == 0:\n",
    "                labels = labels.unsqueeze(0)\n",
    "            labels = labels.to(device)\n",
    "            _, logits = model(img_w)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "\n",
    "accuracy = evaluate(adapted_model, target_loader)\n",
    "print(f\"Adapted Target Domain Accuracy on OrganAMNIST Train Subset: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
