{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms as T, datasets, models\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 65  \n",
    "batch_size = 64\n",
    "source_epochs = 5 \n",
    "adapt_epochs = 5\n",
    "learning_rate = 1e-3\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "weak_transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "strong_transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfficeHomeTargetDataset(Dataset):\n",
    "    def __init__(self, root, transform_weak, transform_strong=None):\n",
    "        self.dataset = datasets.ImageFolder(root, transform=None)\n",
    "        self.transform_weak = transform_weak\n",
    "        self.transform_strong = transform_strong if transform_strong is not None else transform_weak\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        img_w = self.transform_weak(img)\n",
    "        img_s = self.transform_strong(img)\n",
    "        return img_w, img_s, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFUDAModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SFUDAModel, self).__init__()\n",
    "        backbone = models.resnet50(pretrained=True)\n",
    "        # backbone = models.resnet101(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        self.fc = nn.Linear(backbone.fc.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        logits = self.fc(features)\n",
    "        return features, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank:\n",
    "    def __init__(self, feature_dim, dataset_size, momentum=0.2):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.momentum = momentum\n",
    "        self.bank = torch.zeros(dataset_size, feature_dim).to(device)\n",
    "    \n",
    "    def update(self, indices, features):\n",
    "        with torch.no_grad():\n",
    "            self.bank[indices] = self.momentum * self.bank[indices] + (1 - self.momentum) * features\n",
    "            self.bank[indices] = F.normalize(self.bank[indices], dim=1)\n",
    "    \n",
    "    def get_features(self):\n",
    "        return self.bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_training_loss(p_w, p_s, pseudo_label, num_classes, lambda_diversity=0.1, omega_entropy=0.1):\n",
    "    ce_loss = F.cross_entropy(p_w, pseudo_label.long()) + F.cross_entropy(p_s, pseudo_label.long())\n",
    "    mean_pred = p_w.mean(dim=0)\n",
    "    diversity_loss = F.kl_div(mean_pred.log(), torch.full_like(mean_pred, 1/num_classes), reduction='batchmean')\n",
    "    entropy_loss = - (p_w * p_w.log()).sum(dim=1).mean()\n",
    "    return ce_loss + lambda_diversity * diversity_loss + omega_entropy * entropy_loss\n",
    "\n",
    "def contrastive_loss(anchor, positive, negatives, temperature=0.05):\n",
    "    anchor = F.normalize(anchor, dim=1)\n",
    "    positive = F.normalize(positive, dim=1)\n",
    "    negatives = F.normalize(negatives, dim=1)\n",
    "    \n",
    "    pos_sim = torch.sum(anchor * positive, dim=1) / temperature\n",
    "    neg_sim = torch.matmul(anchor, negatives.t()) / temperature\n",
    "    logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)\n",
    "    labels = torch.zeros(anchor.size(0), dtype=torch.long).to(device)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    return loss\n",
    "\n",
    "def mmd_loss(source_features, target_features):\n",
    "    mean_source = source_features.mean(dim=0)\n",
    "    mean_target = target_features.mean(dim=0)\n",
    "    loss = torch.norm(mean_source - mean_target, p=2)**2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(features, pseudo_labels, num_classes):\n",
    "    centroids = []\n",
    "    for c in range(num_classes):\n",
    "        mask = (pseudo_labels == c)\n",
    "        if mask.sum() == 0:\n",
    "            centroid = torch.zeros(features.size(1)).to(device)\n",
    "        else:\n",
    "            centroid = features[mask].mean(dim=0)\n",
    "            centroid = F.normalize(centroid.unsqueeze(0), dim=1).squeeze(0)\n",
    "        centroids.append(centroid)\n",
    "    centroids = torch.stack(centroids, dim=0)\n",
    "    return centroids\n",
    "\n",
    "def assign_pseudo_labels(features, centroids):\n",
    "    features = F.normalize(features, dim=1)\n",
    "    sim = torch.matmul(features, centroids.t())\n",
    "    pseudo_labels = sim.argmax(dim=1)\n",
    "    confidence, _ = sim.max(dim=1)\n",
    "    return pseudo_labels, confidence\n",
    "\n",
    "def divide_samples(confidence, threshold=0.8):\n",
    "    source_like_idx = (confidence >= threshold).nonzero(as_tuple=False).squeeze()\n",
    "    target_specific_idx = (confidence < threshold).nonzero(as_tuple=False).squeeze()\n",
    "    return source_like_idx, target_specific_idx\n",
    "\n",
    "def update_pseudo_labels(model, dataloader, tau=0.8):\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_indices = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (img_w, _, _) in enumerate(dataloader):\n",
    "            img_w = img_w.to(device)\n",
    "            features, _ = model(img_w)\n",
    "            features = F.normalize(features, dim=1)\n",
    "            all_features.append(features)\n",
    "            indices = torch.arange(batch_idx * batch_size, min((batch_idx+1)*batch_size, len(dataloader.dataset))).to(device)\n",
    "            all_indices.append(indices)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_indices = torch.cat(all_indices, dim=0)\n",
    "    pseudo_labels = all_features.argmax(dim=1) \n",
    "    centroids = compute_centroids(all_features, pseudo_labels, num_classes)\n",
    "    pseudo_labels, confidence = assign_pseudo_labels(all_features, centroids)\n",
    "    source_like_idx, target_specific_idx = divide_samples(confidence, threshold=tau)\n",
    "    model.train()\n",
    "    return all_indices, pseudo_labels, confidence, source_like_idx, target_specific_idx, centroids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charanganeshcharanganesh/miniconda3/envs/AI/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/charanganeshcharanganesh/miniconda3/envs/AI/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting source training on Art subset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source Epoch 1/5: 100%|█████████████████████████| 16/16 [10:10<00:00, 38.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Epoch [1/5], Loss: 3.7803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source Epoch 2/5: 100%|█████████████████████████| 16/16 [11:03<00:00, 41.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Epoch [2/5], Loss: 2.6638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source Epoch 3/5: 100%|█████████████████████████| 16/16 [04:23<00:00, 16.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Epoch [3/5], Loss: 2.0236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source Epoch 4/5: 100%|█████████████████████████| 16/16 [01:03<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Epoch [4/5], Loss: 1.6440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source Epoch 5/5: 100%|█████████████████████████| 16/16 [01:05<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Epoch [5/5], Loss: 1.3756\n",
      "Source training finished and model saved.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    art_data_dir = \"../data/Office-Home/Art\"\n",
    "    real_data_dir = \"../data/Office-Home/Real World\"\n",
    "    \n",
    "    full_art_dataset = OfficeHomeTargetDataset(art_data_dir, transform_weak=source_transform, transform_strong=source_transform)\n",
    "    art_subset_indices = list(range(min(1000, len(full_art_dataset))))\n",
    "    art_subset = Subset(full_art_dataset, art_subset_indices)\n",
    "    art_loader = DataLoader(art_subset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    source_model = SFUDAModel(num_classes=num_classes).to(device)\n",
    "    optimizer_source = optim.SGD(source_model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=5e-4)\n",
    "    \n",
    "    print(\"Starting source training on Art subset...\")\n",
    "    for epoch in range(source_epochs):\n",
    "        running_loss = 0.0\n",
    "        for img, _, labels in tqdm(art_loader, desc=f\"Source Epoch {epoch+1}/{source_epochs}\", ncols=80):\n",
    "            img = img.to(device)\n",
    "            labels = labels.to(device)\n",
    "            features, logits = source_model(img)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            optimizer_source.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_source.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Source Epoch [{epoch+1}/{source_epochs}], Loss: {running_loss/len(art_loader):.4f}\")\n",
    "    \n",
    "    \n",
    "    torch.save(source_model.state_dict(), \"../models/source_model_art.pth\")\n",
    "    print(\"Source training finished and model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charanganeshcharanganesh/miniconda3/envs/AI/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/charanganeshcharanganesh/miniconda3/envs/AI/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_8645/128733223.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapted_model.load_state_dict(torch.load(\"../models/source_model_art.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting adaptation on Real World subset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapt Epoch 1/5: 100%|████████████████████████████| 8/8 [01:21<00:00, 10.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapt Epoch [1/5], Loss: 14.5052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapt Epoch 2/5: 100%|████████████████████████████| 8/8 [01:29<00:00, 11.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapt Epoch [2/5], Loss: 15.9446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapt Epoch 3/5: 100%|████████████████████████████| 8/8 [01:23<00:00, 10.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapt Epoch [3/5], Loss: 16.1227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapt Epoch 4/5: 100%|████████████████████████████| 8/8 [01:51<00:00, 13.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapt Epoch [4/5], Loss: 15.4506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adapt Epoch 5/5: 100%|████████████████████████████| 8/8 [01:25<00:00, 10.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapt Epoch [5/5], Loss: 15.4457\n",
      "Adapted Target Domain Accuracy on Real World Subset: 54.80%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    adapted_model = SFUDAModel(num_classes=num_classes).to(device)\n",
    "    adapted_model.load_state_dict(torch.load(\"../models/source_model_art.pth\"))\n",
    "    \n",
    "    for param in adapted_model.fc.parameters():\n",
    "        param.requires_grad = False\n",
    "    full_real_dataset = OfficeHomeTargetDataset(real_data_dir, transform_weak=weak_transform, transform_strong=strong_transform)\n",
    "    real_subset_indices = list(range(min(500, len(full_real_dataset))))\n",
    "    real_subset = Subset(full_real_dataset, real_subset_indices)\n",
    "    real_loader = DataLoader(real_subset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "\n",
    "    dataset_size = len(real_subset)\n",
    "    feature_dim = 2048\n",
    "    memory_bank = MemoryBank(feature_dim, dataset_size, momentum=0.2)\n",
    "    \n",
    "    optimizer_adapt = optim.SGD(adapted_model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=5e-4)\n",
    "    alpha = 1.0  \n",
    "    beta = 0.5   \n",
    "    gamma = 1.0 \n",
    "    dataset_indices = np.arange(dataset_size)\n",
    "    \n",
    "    print(\"Starting adaptation on Real World subset...\")\n",
    "    for epoch in range(adapt_epochs):\n",
    "        all_indices, pseudo_labels, confidence, src_like_idx, tgt_spec_idx, centroids = update_pseudo_labels(adapted_model, real_loader, tau=0.8)\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (img_w, img_s, _) in enumerate(tqdm(real_loader, desc=f\"Adapt Epoch {epoch+1}/{adapt_epochs}\", ncols=80)):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = batch_start + img_w.size(0)\n",
    "            batch_indices = torch.tensor(dataset_indices[batch_start:batch_end]).to(device)\n",
    "            \n",
    "            img_w = img_w.to(device)\n",
    "            img_s = img_s.to(device)\n",
    "            \n",
    "            features_w, logits_w = adapted_model(img_w)\n",
    "            features_s, logits_s = adapted_model(img_s)\n",
    "            p_w = F.softmax(logits_w, dim=1)\n",
    "            p_s = F.softmax(logits_s, dim=1)\n",
    "            batch_pseudo = pseudo_labels[batch_indices]\n",
    "            \n",
    "            loss_self = self_training_loss(p_w, p_s, batch_pseudo, num_classes)\n",
    "            batch_features = F.normalize(features_w, dim=1)\n",
    "            positive_proto = centroids[batch_pseudo]\n",
    "            bank_features = memory_bank.get_features()\n",
    "            \n",
    "            loss_contrastive = 0.0\n",
    "            for i in range(batch_features.size(0)):\n",
    "                neg_mask = (pseudo_labels != batch_pseudo[i])\n",
    "                negatives = bank_features[neg_mask]\n",
    "                if negatives.size(0) > 0:\n",
    "                    loss_contrastive += contrastive_loss(batch_features[i].unsqueeze(0),\n",
    "                                                         positive_proto[i].unsqueeze(0),\n",
    "                                                         negatives)\n",
    "            loss_contrastive = loss_contrastive / batch_features.size(0)\n",
    "            \n",
    "            src_mask = torch.tensor([idx.item() in src_like_idx.cpu().numpy() for idx in batch_indices]).bool()\n",
    "            tgt_mask = torch.tensor([idx.item() in tgt_spec_idx.cpu().numpy() for idx in batch_indices]).bool()\n",
    "            if src_mask.sum() > 0 and tgt_mask.sum() > 0:\n",
    "                src_feats = batch_features[src_mask]\n",
    "                tgt_feats = batch_features[tgt_mask]\n",
    "                loss_mmd = mmd_loss(src_feats, tgt_feats)\n",
    "            else:\n",
    "                loss_mmd = torch.tensor(0.0).to(device)\n",
    "            \n",
    "            loss = alpha * loss_self + gamma * loss_contrastive + beta * loss_mmd\n",
    "            \n",
    "            optimizer_adapt.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_adapt.step()\n",
    "            \n",
    "            memory_bank.update(batch_indices, batch_features.detach())\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Adapt Epoch [{epoch+1}/{adapt_epochs}], Loss: {running_loss/len(real_loader):.4f}\")\n",
    "    \n",
    "    def evaluate(model, dataloader):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for img_w, _, labels in dataloader:\n",
    "                img_w = img_w.to(device)\n",
    "                labels = labels.to(device)\n",
    "                _, logits = model(img_w)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return 100.0 * correct / total\n",
    "    \n",
    "    accuracy = evaluate(adapted_model, real_loader)\n",
    "    print(f\"Adapted Target Domain Accuracy on Real World Subset: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
